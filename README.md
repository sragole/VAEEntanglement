# VAEEntanglement
This repository contains code which explores the question, can an [algorithm](./3QubitVariationalAutoEncoder.ipynb) learn to distinguish quantum states which are known to be distinct in the way they are entangled? For those in a hurry, the answer for 3 qubits is maybe, but this approach should fail in general. My limited understanding is that entanglement witnesses are generically hard, and for mixed states, the problem is NP-hard [1](https://dl.acm.org/citation.cfm?doid=780542.780545). However, it was a fun exercise to teach myself about variational autoencoders.

The main result is linked above (which I'm working on making more accessible), and my generated data is included. This [notebook](./Classifier%20Test.ipynb) has some scratch work on 2 qubit systems.

This work was the result of collaboration between a group of ML-interested physicists at the University of Maryland, Zach Eldredge, who led an [effort](https://github.com/zeldredge/py-nqs) to reproduce code from 10.1126/science.aag2302, and Alireza Seif, who recently posted a [paper](https://arxiv.org/abs/1804.07718) using neural networks to improve experimental detection.

I also using [QuTip](http://qutip.org/index.html), a fantastic simulation tool, to simulate the quantum dynamics and [Tensorflow](https://www.tensorflow.org/) for the machine learning part.

## Introduction
The proper background for the work included here is usually covered in multiple semester long courses in college, so I will not attempt to reproduce that. Instead, I will try to give a brief overview so that the ML people can understand the data and the physics people can understand the machine learning.

## Density matrices and quantum physics
As you may have heard, quantum mechanics is a theory which describes the world outside of normal classical physics. It is hard to see many quantum effects at room temperature but precise control of lasers, good refridgeration, and other techniques have opened up a world of exploration for physicists. Additionally, our understanding of this theory underlies a lot of important technology, including [lasers](https://en.wikipedia.org/wiki/Laser), [transistors](https://en.wikipedia.org/wiki/Transistor), and [precise sensors](https://en.wikipedia.org/wiki/SQUID). Typically, popular science makes a big deal about two effects, superposition and entanglement. I am focusing on the latter.

Without a proper introduction, I will simply state that a [quantum state](https://en.wikipedia.org/wiki/Quantum_state) is full described by its wavefunction, typically written as $|\psi\rangle$. $|\psi\rangle$ contains all the information needed to the average value of measurements and the probability of the outcomes. The magic of entanglement is that one can have a state of many particles which cannot be described by a combination of wavefunctions for the individial particles. They are necessarily correlated and measurements about part of the system will tell you about the rest of it, as well.

Typically what one does is prepare a system of interest many times, make measurements and build up statistics. Those statistics can be compared with the wavefunction we thought we had. This wavefunction approach works perfectly for closed systems (where the system of interest is completely isolated from the outside world). However, if the system were truly closed, we couldn't manipulate or measure it. As an aside, a lot of experimental quantum physics is devoted to making a system as closed a possible. When the system isn't closed, we need a more powerful framework, the [density matrix](https://en.wikipedia.org/wiki/Density_matrix).

Briefly, the density matrix is a generalization of a wavefunction which can handle all the normal quantum things and additionally can deal with open systems. There's a formal process for doing so, but it's sufficient for here to say that since we don't know everything about the outside world, we should allow for some classical randomness in our system. One run we may get $|\psi\rangle$, the next run we may get $|\psi'\rangle$ due to fluctuations in the environment. The density matrix allows us to do this and there's a decomposition of the density matrix onto all the measurements you would make.

That's how we're generating the data here. We start by making a bunch of random density matrices of different types, decomposing them into measurement averages and using those averages as our data vector.

## Autoencoders and latent variables

For those less familiar with [autoencoders](https://en.wikipedia.org/wiki/Autoencoder), they are basically a compression/decompression algorithm. We input some data, run it through a neural network has a section which has a lower dimension than the data, then run it through another neural network and output something which we compare with the original data. While there are other compression algorithms, the reason autoencoders are interesting is the features they learn to keep. The small, middle layer (called the latent space) retains some compressed version of the data which we hope can help us learn something important about the data. Here, we're assuming that the different entanglement classes will have a different representation in the latent space.

One can go a step further and instead of directly compressing and uncompressing, sample from a distribution in the latent space. Here, instead of encoding a data point into a compressed representation, we train the encoder to convert a data point into a mean and standard deviation. Then, we sample from a Gaussian with those properties and decode the sampled data. Here the randomness forces the algorithm to make cleaner distinctions between different types of data and can be used as a sort of clustering technique. Additionally, with the mean and standard deviation for a particular type of data, the decoder now can be used in a generative manner, inputing noise and outputing something that is very close to data. Unfortunately for us, these techniques did not seem to work on the density matrices.
